{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IPIP 50 Items Example\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/psyphh/xifa/blob/master/examples/ipip50.ipynb)\n",
    "\n",
    "In this example, we will demonstrate how to use `xifa` to analyze a [`ipip50`](http://openpsychometrics.org/_rawdata/BIG5.zip) dataset from [openspychometrics](https://openpsychometrics.org/_rawdata/) with both exploratory and confirmatory graded response models (GRMs; [Semejima, 1969](https://link.springer.com/article/10.1007%2FBF03372160)). The `ipip50` dataset include responses of 19719 subjects on 50 items from International Personality Item Pool (IPIP; [Goldberg, 1992](https://doi.org/10.1037/1040-3590.4.1.26)).\n",
    "\n",
    "To open this notebook on [Google Colab](https://colab.research.google.com/), please use [this link](http://colab.research.google.com/github/psyphh/xifa/blob/master/examples/ipip50.ipynb). After logging Colab, don't forget to turn on GPU by\n",
    "```\n",
    "Go to Menu > Runtime > Change runtime > Change hardware acceleration to GPU.\n",
    "```\n",
    "\n",
    "## Environment Setup\n",
    "For first running this notebook, please install `xifa` via\n",
    "```\n",
    "pip install --upgrade xifa\n",
    "```\n",
    "on terminal, or\n",
    "```\n",
    "!pip install --upgrade xifa\n",
    "```\n",
    "on jupyter notebook (or Colab).\n",
    "\n",
    "Let's install `xifa` on jupyter notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:02:51.299731Z",
     "iopub.status.busy": "2022-01-03T03:02:51.299422Z",
     "iopub.status.idle": "2022-01-03T03:03:00.818849Z",
     "shell.execute_reply": "2022-01-03T03:03:00.817865Z",
     "shell.execute_reply.started": "2022-01-03T03:02:51.299664Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade xifa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that `xifa` is established on [`jax`](https://github.com/google/jax), which is officially supported only on Linux and Mac systems. Therefore, if you use a local Windows machine, `xifa` might not work.\n",
    "\n",
    "First of all, we import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:00.820895Z",
     "iopub.status.busy": "2022-01-03T03:03:00.820573Z",
     "iopub.status.idle": "2022-01-03T03:03:02.159772Z",
     "shell.execute_reply": "2022-01-03T03:03:02.158899Z",
     "shell.execute_reply.started": "2022-01-03T03:03:00.820855Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from statsmodels.multivariate.factor_rotation import rotate_factors\n",
    "\n",
    "from xifa import GRM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By default, `jax` uses single precision for floating points arithmetics. To enable double precision arithmetics, try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:02.163136Z",
     "iopub.status.busy": "2022-01-03T03:03:02.162787Z",
     "iopub.status.idle": "2022-01-03T03:03:02.166549Z",
     "shell.execute_reply": "2022-01-03T03:03:02.165723Z",
     "shell.execute_reply.started": "2022-01-03T03:03:02.163099Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "In the next step, we download the `ipip50` dataset, unzip it, and read it as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:02.168634Z",
     "iopub.status.busy": "2022-01-03T03:03:02.168145Z",
     "iopub.status.idle": "2022-01-03T03:03:04.463481Z",
     "shell.execute_reply": "2022-01-03T03:03:04.462630Z",
     "shell.execute_reply.started": "2022-01-03T03:03:02.168599Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_dataset(dataset_url,\n",
    "                     dataset_name):\n",
    "    req = requests.get(dataset_url, allow_redirects=True)\n",
    "    open(dataset_name + '.zip',\"wb\").write(req.content)\n",
    "    zipfile.ZipFile(dataset_name + '.zip').extractall()\n",
    "    os.remove(dataset_name+'.zip')\n",
    "\n",
    "dataset_url = \"http://openpsychometrics.org/_rawdata/BIG5.zip\"\n",
    "dataset_name = \"ipip50\"\n",
    "download_dataset(dataset_url, dataset_name)\n",
    "data = pd.read_csv(\"BIG5/data.csv\", sep=\"\\t\").iloc[:,-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The dataset only includes one observation with missing values (which is coded by `0` so we need to replace it by `np.nan`). However, the observation didn't response any items. Hence, we delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:04.465121Z",
     "iopub.status.busy": "2022-01-03T03:03:04.464726Z",
     "iopub.status.idle": "2022-01-03T03:03:04.511983Z",
     "shell.execute_reply": "2022-01-03T03:03:04.511165Z",
     "shell.execute_reply.started": "2022-01-03T03:03:04.465072Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data.replace(0, np.nan)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If missing data are presented, `xifa` uses the so-called full-information approach for estimation. Therefore, it is unnecessary to explicitly handle `np.nan`. The full-information estimation could still yield a consistent estimator if the assumption of missing at random (MAR) is satisfied ([Rubin, 1976](https://doi.org/10.2307/2335739)).\n",
    "\n",
    "`xifa` requires the ordered responses to be coded from `0` to `max_cats-1`, where `max_cats` is the maximal number of ordered categories. Because the original responses are coded from `1` to `max_cats`, we need to subtract all values by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:04.513618Z",
     "iopub.status.busy": "2022-01-03T03:03:04.513258Z",
     "iopub.status.idle": "2022-01-03T03:03:04.520202Z",
     "shell.execute_reply": "2022-01-03T03:03:04.519222Z",
     "shell.execute_reply.started": "2022-01-03T03:03:04.513571Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that `xifa` could handle items with different numbers of categories. In that case, just code item `i` from `0` to `n_cats_i-1`, where `n_cats_i` is the number of categories of item `i`.\n",
    "\n",
    "We then reverse the coding for the \"negative items\" and use the heat map of correlation matrix to check whether our reversing is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:04.522175Z",
     "iopub.status.busy": "2022-01-03T03:03:04.521720Z",
     "iopub.status.idle": "2022-01-03T03:03:04.760614Z",
     "shell.execute_reply": "2022-01-03T03:03:04.759760Z",
     "shell.execute_reply.started": "2022-01-03T03:03:04.522137Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "negative_items = [\"E2\", \"E4\", \"E6\", \"E8\", \"E10\",\n",
    "                  \"N2\", \"N4\",\n",
    "                  \"A1\", \"A3\", \"A5\", \"A7\",\n",
    "                  \"C2\", \"C4\", \"C6\", \"C8\",\n",
    "                  \"O2\", \"O4\", \"O6\"]\n",
    "data[negative_items] = 4 - data[negative_items]\n",
    "\n",
    "plt.figure(figsize=(3, 3), dpi=150)\n",
    "plt.imshow(data.corr(),\n",
    "           cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exploratory IFA Implementation\n",
    "Now, we conduct exploratory IFA by using the `GRM` object imported from `xifa`. To initialize a `GRM` instance, a dataset (`data`) and a number of factors (`n_factors`) must be specified. The data must be a two dimensional array (`n_cases * n_items`) that can be transformed to `jax.numpy.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:04.763643Z",
     "iopub.status.busy": "2022-01-03T03:03:04.763081Z",
     "iopub.status.idle": "2022-01-03T03:03:13.313995Z",
     "shell.execute_reply": "2022-01-03T03:03:13.313159Z",
     "shell.execute_reply.started": "2022-01-03T03:03:04.763601Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grm = GRM(data=data, \n",
    "          n_factors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After initialization, by default, a summary for model will be printed. This can be turn off by setting `verbose=False`.\n",
    "\n",
    "The marginal maximum likelihood (MML) estimation can be implemented by the `fit()` method. `xifa` implements MML by a vectorized Metropolis-Hastings Robbins-Monro (MH-RM) algorithm. The algorithm includes two stages. The first stage updates the parameter estimate by a stochastic expectation-maximization (StEM) algorithm. The second stage conducts stochastic approximation (SA) to refine the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:13.316000Z",
     "iopub.status.busy": "2022-01-03T03:03:13.315655Z",
     "iopub.status.idle": "2022-01-03T03:03:28.338507Z",
     "shell.execute_reply": "2022-01-03T03:03:28.337564Z",
     "shell.execute_reply.started": "2022-01-03T03:03:13.315964Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grm.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this example, CPU takes about 3-8 minutes to finish the optimization job. If any GPU is available, the job can be speed up. On Colab with GPU, the job takes about 30 seconds (depending on the GPU resource availability).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The MML parameter estimate is stored in the `params` attribute. `params` is a dictionary with three keys: `intercept` for the intercept vector, `loading` for the factor loading matrix, and `corr` for the factor correlation matrix. For example, the estimated factor correlation matrix can be extracted by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:28.340420Z",
     "iopub.status.busy": "2022-01-03T03:03:28.339898Z",
     "iopub.status.idle": "2022-01-03T03:03:28.487257Z",
     "shell.execute_reply": "2022-01-03T03:03:28.486443Z",
     "shell.execute_reply.started": "2022-01-03T03:03:28.340355Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.round(grm.params[\"corr\"], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The factor correlations are all zero because the exploratory analysis assumes orthogonal factors by convention. To capture the relationships among items and factors appropriately, we use `rotate_factors` in `statsmodels` to obtain a rotated loading matrix and factor correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:28.488896Z",
     "iopub.status.busy": "2022-01-03T03:03:28.488565Z",
     "iopub.status.idle": "2022-01-03T03:03:30.568300Z",
     "shell.execute_reply": "2022-01-03T03:03:30.567473Z",
     "shell.execute_reply.started": "2022-01-03T03:03:28.488859Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "L, T  = rotate_factors(\n",
    "    grm.params[\"loading\"],\n",
    "    'quartimin', 0.5)\n",
    "print(\"Rotated Factor Loading Matrix: \\n\",\n",
    "      np.round(L, 2))\n",
    "print(\"Factor Correlation Matrix: \\n\",\n",
    "      np.round(T.T @ T, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The optimization history is stored in the `trace` attribute. To see how the complete data loss changes over iterations, we may use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:30.569906Z",
     "iopub.status.busy": "2022-01-03T03:03:30.569546Z",
     "iopub.status.idle": "2022-01-03T03:03:30.706448Z",
     "shell.execute_reply": "2022-01-03T03:03:30.705632Z",
     "shell.execute_reply.started": "2022-01-03T03:03:30.569866Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(grm.trace[\"closs\"])\n",
    "plt.ylabel('Complete Loss Values')\n",
    "plt.xlabel('Iteration')\n",
    "plt.title('MH-RM History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Confirmatory IFA Implementation\n",
    " Finally, we conduct a confirmatory analysis with GRM. For a confirmatory analysis, an additional argument `patterns` must be given. `patterns` is a `dict` (`python` dictionary) with two keys: `loading` and `corr`. `patterns[\"loading\"]` is also a `dict` to specify the relationships among items and factors. On the other hand, `patterns[\"corr\"]` is for the pattern of factor correlations. If either `loading` or `corr` is not included in `pattern`, its corresponding parameter matrix will be freely estimated.\n",
    "\n",
    " For example, we can set an independent cluster structure for the loading matrix in this `ipip50` example by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:30.708286Z",
     "iopub.status.busy": "2022-01-03T03:03:30.707967Z",
     "iopub.status.idle": "2022-01-03T03:03:30.716652Z",
     "shell.execute_reply": "2022-01-03T03:03:30.715471Z",
     "shell.execute_reply.started": "2022-01-03T03:03:30.708253Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_factors = 5\n",
    "n_items = 50\n",
    "items_per_factor = int(n_items / n_factors)\n",
    "patterns = {\n",
    "    \"loading\":{m: list(\n",
    "        range(m * items_per_factor, (m + 1) * items_per_factor)) for m in range(n_factors)}}\n",
    "print(patterns[\"loading\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that `patterns[\"loading\"]` specifies the relationship among items and factors through their indices (starting form 0) by a `dict` (`key` for indices of factors and `value` for indices of corresponding items). Note that we didn't specify a `patterns[\"corr\"]` and hence all of the correlations among factors will be freely estimated.\n",
    "\n",
    "If `patterns` is specified when initializing `GRM`, the model will be treated as confirmatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:30.718610Z",
     "iopub.status.busy": "2022-01-03T03:03:30.718151Z",
     "iopub.status.idle": "2022-01-03T03:03:31.380136Z",
     "shell.execute_reply": "2022-01-03T03:03:31.379093Z",
     "shell.execute_reply.started": "2022-01-03T03:03:30.718557Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grm = GRM(data=data, \n",
    "          n_factors=5,\n",
    "          patterns=patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can still use `fit()` method to calculate a parameter estimate, i.e.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:31.382540Z",
     "iopub.status.busy": "2022-01-03T03:03:31.381361Z",
     "iopub.status.idle": "2022-01-03T03:03:47.023248Z",
     "shell.execute_reply": "2022-01-03T03:03:47.022186Z",
     "shell.execute_reply.started": "2022-01-03T03:03:31.382494Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grm.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The corresponding estimated loading matrix and correlation matrix are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-03T03:03:47.025170Z",
     "iopub.status.busy": "2022-01-03T03:03:47.024737Z",
     "iopub.status.idle": "2022-01-03T03:03:47.041311Z",
     "shell.execute_reply": "2022-01-03T03:03:47.039463Z",
     "shell.execute_reply.started": "2022-01-03T03:03:47.025127Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Factor Loading Matrix: \\n\",\n",
    "      np.round(grm.params[\"loading\"], 2))\n",
    "print(\"Factor Correlation Matrix: \\n\",\n",
    "      np.round(grm.params[\"corr\"], 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
